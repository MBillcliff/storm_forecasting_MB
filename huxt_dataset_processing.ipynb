{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbb787d-c0f3-484c-bb5e-a462baa91e4b",
   "metadata": {},
   "source": [
    "#Â HUXt Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb7139-96f8-4617-8914-f103784bc223",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffaaaa; padding: 15px; border-radius: 5px;\">\n",
    "<b>Important Note:</b> Must create HUXt dataframes from \"ambient_huxt.ipynb\" first.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5fa56-ca26-4a96-873e-9f6c47fc1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011c723-d06b-46f2-9afa-fbb8461b8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "huxt_utils_dir = os.path.join(os.getcwd(), 'src', 'huxt')\n",
    "ml_utils_dir = os.path.join(os.getcwd(), 'src', 'ml')\n",
    "data_dir = os.path.join(os.getcwd(), 'src', 'data')\n",
    "\n",
    "# Add my utils to the path\n",
    "import sys\n",
    "sys.path.append(huxt_utils_dir)\n",
    "sys.path.append(ml_utils_dir)\n",
    "\n",
    "from data_loader import load_huxt_data_as_windows, load_omni_data\n",
    "import huxt_utils as HU\n",
    "import fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aeba00-c51f-4763-b23d-3efda2ffcae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Specify which CRs \n",
    "start_cr = 1892   # Min 1892\n",
    "end_cr = 2290     # Max 2290\n",
    "\n",
    "additional_cols = ['hp30', 'velocity gradient']\n",
    "folder_name = f'HUXt1'\n",
    "print(f'Processing {folder_name}')\n",
    "for cr in range(start_cr, end_cr):\n",
    "    df = HU.huxt_output_to_ml_df(rotation_number=cr, extra_columns=additional_cols,\n",
    "                                 folder_name=folder_name, save=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf28be-59eb-4a03-bee8-10fcdbbdf22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 20  # Process [chunk_size] files at a time\n",
    "output_file = os.path.join(data_dir, 'HUXt', f'{folder_name}_modified', 'full_df.parquet')\n",
    "huxt_data_dir = os.path.join(data_dir, 'HUXt', f'{folder_name}_modified')\n",
    "\n",
    "dfs = []  \n",
    "last_index = None  # Keep track of last index to remove overlap\n",
    "Nens = 100         # Specify no. ensembles\n",
    "\n",
    "OMNI = load_omni_data(data_dir)\n",
    "\n",
    "def process_chunk():\n",
    "    \"\"\"Saves the current chunk of data, removing duplicates and appending to the Parquet file.\"\"\"\n",
    "    global dfs, last_index  # Ensure we modify the global list and last index\n",
    "    \n",
    "    if not dfs:  # If there's no data, skip saving\n",
    "        return\n",
    "\n",
    "    # Concatenate and remove duplicates\n",
    "    chunk_df = pd.concat(dfs, ignore_index=False)\n",
    "\n",
    "    # Drop duplicated indices while keeping the last occurrence\n",
    "    chunk_df = chunk_df[~chunk_df.index.duplicated(keep='last')]\n",
    "\n",
    "    # Remove overlap with the last processed chunk\n",
    "    if last_index is not None:\n",
    "        chunk_df = chunk_df.loc[chunk_df.index > last_index]\n",
    "\n",
    "    # Update last index for next batch\n",
    "    last_index = chunk_df.index[-1]\n",
    "    \n",
    "    OMNI_half_hourly = OMNI.reindex(chunk_df.index, method='ffill')\n",
    "\n",
    "    v_columns = chunk_df.columns[:Nens]\n",
    "    v_grad_columns = chunk_df.columns[Nens:2*Nens]\n",
    "    remainder = chunk_df.columns[2*Nens:]\n",
    "    \n",
    "    chunk_df = pd.concat((chunk_df, OMNI_half_hourly), axis=1)\n",
    "    \n",
    "    v_minus_omni = pd.DataFrame(\n",
    "        chunk_df[v_columns].values - chunk_df['Velocity'].values[:, None],\n",
    "        columns=[f'v_minus_omni_{i}' for i in range(Nens)],\n",
    "        index=chunk_df.index\n",
    "    )\n",
    "    \n",
    "    chunk_df = pd.concat((chunk_df, v_minus_omni), axis=1)\n",
    "\n",
    "    v_minus_omni_columns = v_minus_omni.columns\n",
    "    \n",
    "    # Interleave the columns\n",
    "    interleaved_columns = []\n",
    "    for a_col, b_col, c_col in zip(v_columns, v_grad_columns, v_minus_omni_columns):\n",
    "        interleaved_columns.extend([a_col, b_col, c_col])\n",
    "    interleaved_columns.extend(remainder)\n",
    "    \n",
    "    # Reorder the DataFrame using the new column order\n",
    "    chunk_df_rearranged = chunk_df[interleaved_columns]\n",
    "\n",
    "    # Append to output file using fastparquet\n",
    "    fastparquet.write(output_file, chunk_df_rearranged, compression=\"snappy\", append=os.path.exists(output_file))\n",
    "\n",
    "    # Clear memory\n",
    "    dfs.clear()\n",
    "    print(f\"Saved batch at CR {cr}\")\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    # Main processing loop\n",
    "    for i in range(start_cr, end_cr + 1, chunk_size):\n",
    "        for cr in range(i, min(i + chunk_size, end_cr + 1)):\n",
    "            file_path = os.path.join(huxt_data_dir, f'HUXt_rotation_{cr}')\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                dfs.append(df)\n",
    "            except:\n",
    "                print(f'file for CR {cr} not created')\n",
    "    \n",
    "        process_chunk()\n",
    "    \n",
    "    print('Done')\n",
    "\n",
    "else:\n",
    "    print(f'File exists at {output_file}\\nDelete old file first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d6bb2-b866-4459-9273-7c4cd90fa33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Check that we have processed correctly\n",
    "\n",
    "print('File location:', output_file)\n",
    "df = pd.read_parquet(output_file, engine=\"fastparquet\")\n",
    "\n",
    "# Check if dataset has unique indices\n",
    "print('Unique indices:', len(df.index) == len(set(df.index)))\n",
    "\n",
    "# Columns should read: v_0, v_0_gradient, v_minus_omni_0, v_1, ... , v_99, v_99_gradient, v_minus_omni_99, hpo\n",
    "print('Data points:', len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b951d-bae9-40be-bb20-9d64960a7cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
