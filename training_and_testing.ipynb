{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b172dfb8-46ec-49b8-88b3-7a860d9f3e19",
   "metadata": {},
   "source": [
    "# Model Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d71273-8f4d-434a-b58a-5cc611785c0c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679e70a-20f3-4f99-bcb0-85df53de1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bfc27-f383-438b-a3e0-f2554f3dc16c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sunpy.coordinates.sun import carrington_rotation_number, carrington_rotation_time\n",
    "from matplotlib.colors import Normalize\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'src', 'data')\n",
    "huxt_utils_dir = os.path.join(os.getcwd(), 'src', 'huxt')\n",
    "ml_utils_dir = os.path.join(os.getcwd(), 'src', 'ml')\n",
    "\n",
    "# Add my utils to the path\n",
    "import sys\n",
    "sys.path.append(huxt_utils_dir)\n",
    "sys.path.append(ml_utils_dir)\n",
    "\n",
    "# Import my own modules from utils\n",
    "\n",
    "from ensemble_analysis import evaluate_predictions\n",
    "from data_loader import load_huxt_data_as_windows, load_omni_data\n",
    "from data_functions import convert_hpo_to_boolean, train_test_val_split, balance_data\n",
    "from ensemble_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618780a-7658-4267-b293-435c901d3073",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10706ad-465a-4091-9a6a-ffb50262f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(f, run_number, target, \n",
    "         input_window_size, input_buffer_size,\n",
    "         output_window_size, post_window_size, \n",
    "         variables, balance, nens, scale_p, model_class, \n",
    "         final_name, HUXt_dir, X_windows, y_windows, times, \n",
    "         subfolder, seed):\n",
    "    \"\"\"\n",
    "    Call this function to make a forecast model using the given input parameters\n",
    "\n",
    "    input: \n",
    "    - f                   : file - metric table file\n",
    "    - run_number          : int - id for saving model outputs\n",
    "    - target              : str - target variable name ('hp30' or 'hp60')\n",
    "    - input_window_size   : int - length of input window (in hours)\n",
    "    - input_buffer_size   : int - length of buffer zone (in hours)\n",
    "    - output_window_size  : int - length of output window (in hours)\n",
    "    - post_window_size    : int - length of post window (for plotting)\n",
    "    - variables           : np.array - array of variable names to include in the model\n",
    "    - balance             : bool - balances storm and non-storm output windows when True\n",
    "    - nens                : int - number of HUXt ensemble to use\n",
    "    - scale_p             : bool - scales ensemble probabilities when True\n",
    "    - model_class         : sklearn model - Classification model from sklearn\n",
    "    - final_name          : string - model name for final_classifier\n",
    "    - HUXt_dir            : os.path - Path to the HUXt data directory\n",
    "    - X_windows           : windows of training parameters\n",
    "    - y_windows           : windows of target variable\n",
    "    - times               : times corresponding to the windows\n",
    "    - subfolder           : str - name of the subfolder to save plots to \n",
    "    - seed                : int - random seed for numpy\n",
    "    \"\"\"\n",
    "\n",
    "    if final_name in ['persistence', '27_day_persistence']:\n",
    "        nens = 1\n",
    "    \n",
    "    # Get the cadence factor of the dataset\n",
    "    df = pd.read_parquet(os.path.join(HUXt_dir, 'HUXt_rotation_1892'))\n",
    "    \n",
    "    # Set constants:\n",
    "    save_dir = subfolder\n",
    "    main_dir = os.getcwd()\n",
    "\n",
    "    # Balance the data\n",
    "    if balance: \n",
    "        X_windows_balanced, y_windows_balanced, times_balanced = balance_data(X_windows, y_windows, times, input_window_size=input_window_size, post_window_size=post_window_size, storm_threshold=storm_training_threshold)\n",
    "        y = convert_hpo_to_boolean(y_windows_balanced, input_window_size=input_window_size, post_window_size=post_window_size, storm_threshold=storm_training_threshold)\n",
    "        # Split to train, val and test sets\n",
    "        X_train, X_val, X_test, y_train_hpo, y_val_hpo, y_test_hpo, times_train, times_val, times_test = train_test_val_split(X_windows_balanced, \n",
    "                                                                                                                                 y_windows_balanced, \n",
    "                                                                                                                                 times_balanced,\n",
    "                                                                                                                                 seed)\n",
    "        \n",
    "    else: \n",
    "        y = convert_hpo_to_boolean(y_windows, input_window_size=input_window_size, post_window_size=post_window_size)\n",
    "        X_train, X_val, X_test, y_train_hpo, y_val_hpo, y_test_hpo, times_train, times_val, times_test = train_test_val_split(X_windows, \n",
    "                                                                                                                                 y_windows, \n",
    "                                                                                                                                 times,\n",
    "                                                                                                                                 seed)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))                                                                                                                          \n",
    "    def refactor_windows(X, y_hpo, name=''): \n",
    "        X[:,:,input_window_size-input_buffer_size:,-1] = 0   \n",
    "\n",
    "        # Remove post_window from our input variables\n",
    "        X_without_post = X[:,:,:-post_window_size]\n",
    "        \n",
    "        # Extract whether there was a storm or not based on hp60 values in the output window\n",
    "        y_bool = convert_hpo_to_boolean(y_hpo, input_window_size=input_window_size, post_window_size=post_window_size)\n",
    "\n",
    "        # Get target var during output window\n",
    "        y = y_bool\n",
    "        \n",
    "        # Combine dimensions for rescaling\n",
    "        X_reshaped = X_without_post.reshape(-1, X_without_post.shape[-1])\n",
    "        \n",
    "        # Scale the data and put it back to original shape\n",
    "        scaled_X = scaler.fit_transform(X_reshaped).reshape(X_without_post.shape)\n",
    "\n",
    "        # Combine last 2 dimensions\n",
    "        scaled_X_reshaped = scaled_X.reshape(scaled_X.shape[:-2] + (-1,))\n",
    "\n",
    "        # Remove (V - OMNI) for buffer and output window\n",
    "        scaled_X_reshaped = scaled_X_reshaped[:, :, :-(input_buffer_size + output_window_size)]\n",
    "\n",
    "        X_hpo = y_hpo[:, :input_window_size-input_buffer_size] if target in variables else None\n",
    "\n",
    "        print(name, 'ensemble input shape', scaled_X_reshaped.shape)\n",
    "\n",
    "        return X, scaled_X, scaled_X_reshaped, X_hpo, y, y_bool\n",
    "\n",
    "    def remove_small_storms(X, y_hpo, times, storm_testing_threshold):\n",
    "        ''' Removes storms above training threshold and below testing threshold '''\n",
    "        # get the max hpo value for each output window\n",
    "        y_max_hpo = np.max(y_hpo[:, input_window_size:-post_window_size], axis=1)\n",
    "\n",
    "        #Â Extract indices for when we exceed large storm threshold and when we don't exceed storm threshold\n",
    "        storm_indices = np.where(y_max_hpo >= storm_testing_threshold)[0]\n",
    "        non_storm_indices = np.where(y_max_hpo < storm_training_threshold)[0]\n",
    "        \n",
    "        # Randomly drop non-storms to balance with the storm times\n",
    "        non_storm_indices = np.random.choice(non_storm_indices, size=len(storm_indices), replace=False)\n",
    "\n",
    "        # Combine indices\n",
    "        all_indices = np.concatenate((storm_indices, non_storm_indices))\n",
    "\n",
    "        # Extract correct parts of our arrays\n",
    "        X_removed = X[all_indices]\n",
    "        y_hpo_removed = y_hpo[all_indices]\n",
    "        times_removed = times[all_indices]\n",
    "\n",
    "        return X_removed, y_hpo_removed, times_removed\n",
    "    \n",
    "    def get_maes(X):\n",
    "        X_input = X[:, :, :input_window_size - input_buffer_size, -1]\n",
    "        maes = np.mean(np.abs(X_input), axis=-1)\n",
    "        return maes\n",
    "\n",
    "    # Remove storms based on testing threshold\n",
    "    if storm_testing_threshold != storm_training_threshold:\n",
    "        X_test, y_test_hpo, times_test = remove_small_storms(X_test, y_test_hpo, times_test, storm_testing_threshold=storm_testing_threshold)\n",
    "\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    # Extract arrays needed \n",
    "    print('Refactoring...')\n",
    "    X_train, scaled_X_train, scaled_X_train_reshaped, X_train_hpo, y_train, y_train_bool = refactor_windows(X_train, y_train_hpo, 'train')\n",
    "    X_val, scaled_X_val, scaled_X_val_reshaped, X_val_hpo, y_val, y_val_bool = refactor_windows(X_val, y_val_hpo, 'validation')\n",
    "    X_test, scaled_X_test, scaled_X_test_reshaped, X_test_hpo, y_test, y_test_bool = refactor_windows(X_test, y_test_hpo, 'test')\n",
    "    \n",
    "\n",
    "    print(scaled_X_train_reshaped.shape, y_train.shape)\n",
    "    # Create ensemble models\n",
    "    model_params = {'max_iter':2000}\n",
    "    print(\"Training Classifiers...\")\n",
    "    model_array = create_ensemble_models(scaled_X_train_reshaped, X_train_hpo, y_train, model_class, model_params)\n",
    "\n",
    "    # Make ensemble predictions\n",
    "    print('Making ensemble predictions...')\n",
    "    \n",
    "    train_predictions = make_ensemble_predictions(scaled_X_train_reshaped, X_train_hpo, y_train, model_array, predict_probabilities=True)\n",
    "    test_predictions = make_ensemble_predictions(scaled_X_test_reshaped, X_test_hpo, y_test, model_array, predict_probabilities=True)\n",
    "    val_predictions = make_ensemble_predictions(scaled_X_val_reshaped, X_val_hpo, y_val, model_array, predict_probabilities=True)\n",
    "\n",
    "    # Decide whether to sort final_classifier input by MAE\n",
    "    if final_name in ['logreg_sorted', 'attention_NN'] or final_name[:11] == 'logreg_top_':\n",
    "        sort=True\n",
    "    else:\n",
    "        sort=False\n",
    "\n",
    "    # Find MAES for input window\n",
    "    train_maes = get_maes(X_train)\n",
    "    val_maes = get_maes(X_val)\n",
    "    test_maes = get_maes(X_test)\n",
    "\n",
    "    if sort: \n",
    "        # Sort the arrays by their associated MAE\n",
    "        train_indices = np.argsort(train_maes, axis=1)\n",
    "        val_indices = np.argsort(val_maes, axis=1)\n",
    "        test_indices = np.argsort(test_maes, axis=1)\n",
    "        \n",
    "        sorted_train_predictions = np.take_along_axis(train_predictions, train_indices, axis=1)\n",
    "        sorted_val_predictions = np.take_along_axis(val_predictions, val_indices, axis=1)\n",
    "        sorted_test_predictions = np.take_along_axis(test_predictions, test_indices, axis=1)\n",
    "        sorted_train_maes = np.take_along_axis(train_maes, train_indices, axis=1)\n",
    "        sorted_val_maes = np.take_along_axis(val_maes, val_indices, axis=1)\n",
    "        sorted_test_maes = np.take_along_axis(test_maes, test_indices, axis=1)\n",
    "\n",
    "        train_input = [sorted_train_predictions, sorted_train_maes]\n",
    "        val_input = [sorted_val_predictions, sorted_val_maes]\n",
    "        test_input = [sorted_test_predictions, sorted_test_maes]\n",
    "\n",
    "    else:\n",
    "        train_input = [train_predictions, train_maes]\n",
    "        val_input = [val_predictions, val_maes]\n",
    "        test_input = [test_predictions, test_maes]\n",
    "\n",
    "    # Pass hpo for the input window \n",
    "    if final_name == 'persistence':\n",
    "        test_input = y_test_hpo[:, :input_window_size-input_buffer_size]\n",
    "\n",
    "    if final_name == '27_day_persistence':\n",
    "        # Pass the times corresponding to the output window\n",
    "        test_input = times_test[:, input_window_size:-post_window_size]\n",
    "\n",
    "\n",
    "    #Â Train final classifier\n",
    "    print('Training final classifier...')\n",
    "    final_classifier = train_final_classifier(train_input, y_train, final_name)\n",
    "\n",
    "    #Â Make probabilistic predictions\n",
    "    print('Making final forecasts...')\n",
    "    probabilistic_predictions = make_final_classifier_predictions(test_input, final_classifier, final_name, scale=scale_p)\n",
    "    \n",
    "    res = evaluate_predictions(probabilistic_predictions, y_test)\n",
    "    \n",
    "    # Write metrics to file\n",
    "    f.write('\\n')\n",
    "    f.write('-'.join(variables))\n",
    "    cadence_factor = 2\n",
    "    f.write(f',{input_window_size//cadence_factor},{input_buffer_size//cadence_factor},{output_window_size//cadence_factor},{balance},{n_ensembles},{sort},{scale_p},{final_name},{storm_testing_threshold},')\n",
    "    f.write(','.join([str(i) for i in res.values()]))\n",
    "\n",
    "    # Make plots from model output\n",
    "    tag = f\"i{input_window_size}_o{output_window_size}_s{stride}_buff{input_buffer_size}_{'_'.join(variables)}_nens{nens}_sorted_{sort}_scaled_{scale_p}_final_{final_name}_thresh_{storm_testing_threshold}\"\n",
    "        \n",
    "    test_tupe = (probabilistic_predictions, X_test, y_test, y_test_hpo, times_test, train_predictions, train_maes, final_name)\n",
    "\n",
    "    print('Done')\n",
    "    return test_tupe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd403a5a-94bb-4d82-bf07-a0ea9b4a1d3d",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd3f19-96a1-4d28-9f40-e3331e5c8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in OMNI solar wind flow speed data (for visual comparisons only)\n",
    "OMNI = load_omni_data(data_dir)\n",
    "\n",
    "# Params for \n",
    "run_number = 1\n",
    "HUXt_run_number = '1'\n",
    "\n",
    "# Number of ensembles in the specified HUXt dataset\n",
    "ensemble_choice = 100\n",
    "\n",
    "#Â Window Sizes (48 -> 24 hours, 98 -> 49 hours etc. )\n",
    "output_window_size = 48          # Period of which to forecast for\n",
    "input_window_size = 98           # Total amount of data preceding output window (including the buffer)\n",
    "input_buffer_size = 2            # Lead time for the forecast\n",
    "post_window_size = 24            # Data to include succeeding forecast window\n",
    "stride = output_window_size + 1  # Time between starts of successive windows\n",
    "\n",
    "variables = ['velocity', 'gradient', 'v_minus_omni', 'target']  # Variables for training\n",
    "\n",
    "n_ensembles = 100 # Max = no. ensembles in HUXt database\n",
    "\n",
    "target = 'hp30'  # Target variable (must be 'hp30')\n",
    "\n",
    "balance = True   #Â Whether to balance storm and non-storm\n",
    "scale = True     # Whether to scale logreg output\n",
    "\n",
    "model_class = LogisticRegression # Type of ensemble classifier\n",
    "final_name = 'weighted_mean'     # Type of final classifier\n",
    "\n",
    "start_cr = 1892   # Min = 1892\n",
    "end_cr =2278     # Max = 2278\n",
    "\n",
    "# Set training and testing thresholds for Hp30 index\n",
    "storm_training_threshold = 4.66\n",
    "storm_testing_threshold = 4.66\n",
    "\n",
    "random_seed = 151201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c3db9-52e6-4525-96ce-3a371e497375",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "A metric table for this test will be stored at src/figures/metric_tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d437f8-ec04-4633-a2f6-b5da238a8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To overwrite a folder, you can change the following 'False' to 'True'\n",
    "# Make sure to click run, then change back to 'False' to prevent unwanted overwrites \n",
    "OVERWRITE = False\n",
    "\n",
    "# Define save name\n",
    "leading_zero = 0 if run_number < 10 else ''\n",
    "save_name = f'run_{leading_zero}{run_number}'\n",
    "\n",
    "# Setup data directories\n",
    "huxt_data_dir = os.path.join(os.getcwd(), 'src', 'data', 'huxt', f'HUXt{HUXt_run_number}_modified')\n",
    "metric_table_dir = os.path.join(os.getcwd(), 'src', 'figures', 'metric_tables', f'{save_name}_metric_table.csv')\n",
    "figure_dir = os.path.join(os.getcwd(), 'src', 'figures')\n",
    "\n",
    "# replace 'target' with target index name\n",
    "if 'target' in variables: \n",
    "    variables.append(target)\n",
    "    variables.remove('target')\n",
    "\n",
    "# Clear metric table\n",
    "FIRST_WRITE = True\n",
    "\n",
    "if OVERWRITE:\n",
    "    # If we overwrite, we must set 'FIRST_WRITE = True'\n",
    "    FIRST_WRITE = True\n",
    "    with open(metric_table_dir, 'w') as f:\n",
    "        pass\n",
    "\n",
    "\n",
    "with open(metric_table_dir, 'a') as f:\n",
    "    if FIRST_WRITE:\n",
    "        f.write('Variables,Input Window Size (hours),Input Buffer Size (hours),Output Window Size (hours),Balanced,N_ensembles,Sorted,Scaled,Final Classifier,Storm Test Threshold,')\n",
    "        #Â Use dummy values to get metric names\n",
    "        f.write(','.join(evaluate_predictions(np.array([[1], [0]]), np.array([[1], [0]])).keys()))\n",
    "        FIRST_WRITE = False\n",
    "        \n",
    "    # Load in data\n",
    "    print(f'LOADING CR {start_cr} TO {end_cr}...')\n",
    "    X_windows, y_windows, times = load_huxt_data_as_windows(huxt_data_dir,\n",
    "        target, \n",
    "        input_window_size=input_window_size, \n",
    "        output_window_size=output_window_size, \n",
    "        post_window_size=post_window_size, \n",
    "        stride=stride,\n",
    "        start_cr=start_cr,\n",
    "        end_cr=end_cr,\n",
    "        n_ensembles=n_ensembles,\n",
    "        ensemble_choice=ensemble_choice,\n",
    "        )\n",
    "\n",
    "    # Run training and testing function\n",
    "    output = main(f=f,\n",
    "         run_number=run_number, \n",
    "         target=target, \n",
    "         input_window_size=input_window_size, \n",
    "         output_window_size=output_window_size,\n",
    "         post_window_size=post_window_size,\n",
    "         input_buffer_size=input_buffer_size,\n",
    "         variables=variables,\n",
    "         balance=balance,\n",
    "         model_class=model_class,\n",
    "         final_name=final_name,\n",
    "         HUXt_dir=huxt_data_dir,\n",
    "         X_windows=X_windows, \n",
    "         y_windows=y_windows,\n",
    "         times=times,\n",
    "         nens=n_ensembles,\n",
    "         scale_p=scale,\n",
    "         subfolder=save_name,\n",
    "         seed=random_seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce15ca-6cbf-4f36-ad2d-ae191fdb882c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
